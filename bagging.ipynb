{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from train import get_dataloaders, train_model, calculate_f1\n",
    "\n",
    "import logging\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_data = pd.read_csv(data_dir + 'train_data.csv')\n",
    "#augmented_train_data = pd.read_csv(data_dir + 'augmented_data_label_1_pegasus.csv')\n",
    "augmented_train_data = pd.read_csv(data_dir + 'augmented_data_label_1_parrot.csv')\n",
    "train_data = pd.concat([original_train_data, augmented_train_data], axis=0)\n",
    "\n",
    "test_data = pd.read_csv(data_dir + 'test_data.csv')\n",
    "final_pred_data = pd.read_csv(data_dir + 'final_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 balanced sub-datasets for bagging\n",
    "num_sub_datasets = 5\n",
    "train_data_list = []\n",
    "\n",
    "ones = train_data[train_data['label'] == 1]\n",
    "zeros = train_data[train_data['label'] == 0]\n",
    "num = len(ones)\n",
    "\n",
    "for i in range(num_sub_datasets):\n",
    "    train_data_list.append(pd.concat([ones, zeros.sample(n=num, random_state=i)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there's any null value in the text column\n",
    "test_data[test_data['text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "USE_LR_SCHEDULER = False\n",
    "PRETRAINED_MODEL_NAME = \"microsoft/deberta-base\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "args = {\n",
    "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "    \"USE_LR_SCHEDULER\": USE_LR_SCHEDULER,\n",
    "    \"PRETRAINED_MODEL_NAME\": PRETRAINED_MODEL_NAME,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels using the 5 models (voting)\n",
    "def predict(model_names, test_dataloader):\n",
    "    y_pred_list = []\n",
    "    for name in model_names:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(name, num_labels=2)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        # The list of predictions for each model\n",
    "        y_pred = []\n",
    "        for batch in test_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            # Extend every batch\n",
    "            y_pred.extend(logits.argmax(-1).cpu().numpy())\n",
    "        # Now we have a list of list of predictions\n",
    "        y_pred_list.append(y_pred)\n",
    "    # Voting\n",
    "    y_pred = np.array(y_pred_list).T\n",
    "    y_pred = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=y_pred)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def calculate_bagging_f1(model_names, val_dataloader):\n",
    "    # Get the predictions\n",
    "    y_pred = predict(model_names, val_dataloader)\n",
    "    # Get the labels\n",
    "    y_true = np.array([])\n",
    "    for batch in val_dataloader:\n",
    "        y_true = np.concatenate((y_true, batch['labels'].numpy()))\n",
    "    # Calculate the f1 score\n",
    "    metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "    result_dict = metric.compute(predictions=y_pred, references=y_true)\n",
    "    return result_dict[\"f1\"]\n",
    "\n",
    "# If you need to load the models from disk here's your helper\n",
    "def load_models(num_models: int, device: torch.device):\n",
    "    model_list = []\n",
    "    for i in range(num_models):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(f\"bagging_model_{i+1}\", num_labels=2)\n",
    "        model.to(device)\n",
    "        model_list.append(model)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 5 models...\n",
      "Training model 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function get_dataloaders.<locals>.tokenize_function at 0x7fc4f000e8b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32799445b5d14890b387f9c256024338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.3229433596134186: 100%|██████████| 525/525 [12:36<00:00,  1.44s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.029009774327278137: 100%|██████████| 525/525 [12:34<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a38289cc1e45e5839293314e94b412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.19221924245357513: 100%|██████████| 525/525 [10:56<00:00,  1.25s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.17639996111392975: 100%|██████████| 525/525 [12:15<00:00,  1.40s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac81a027d964552a7bd3264346b641b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.2260514497756958: 100%|██████████| 525/525 [13:36<00:00,  1.56s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.3389781415462494: 100%|██████████| 525/525 [13:41<00:00,  1.56s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fad7d3239c485c8cdc95e7a09ead46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.057452332228422165: 100%|██████████| 525/525 [13:38<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.004271307028830051: 100%|██████████| 525/525 [13:24<00:00,  1.53s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19293feb5b63448891c9b3ac51f870da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.6198453307151794: 100%|██████████| 525/525 [13:22<00:00,  1.53s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss: 0.14537973701953888: 100%|██████████| 525/525 [13:34<00:00,  1.55s/it] \n"
     ]
    }
   ],
   "source": [
    "# Construct our \"bags\"\n",
    "print('Training {} models...'.format(num_sub_datasets))\n",
    "for i in range(num_sub_datasets):\n",
    "    print('Training model {}...'.format(i+1))\n",
    "    train_dataloader, _ = get_dataloaders(args, train_data_list[i], None)\n",
    "    train_model(args, device, train_dataloader, model_name = \"bagging_model_{}\".format(i+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting F1 score fot Test Data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc8a401b13a4398a550770d0439bdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate f1 score\n",
      "[0.5075075075075075, 0.5166959578207381, 0.5189189189189188, 0.5419354838709677, 0.5576519916142558]\n",
      "Bagging f1 score\n",
      "0.5489443378119002\n"
     ]
    }
   ],
   "source": [
    "_, test_dataloader = get_dataloaders(args, None, test_data)\n",
    "\n",
    "print(\"Separate f1 score\")\n",
    "list_of_metrics = []\n",
    "for i in range(num_sub_datasets):\n",
    "    metric = calculate_f1(f\"bagging_model_{i+1}\", device, test_dataloader)\n",
    "    list_of_metrics.append(metric)\n",
    "print(list_of_metrics)\n",
    "print(\"Bagging f1 score\")\n",
    "model_names = [\"bagging_model_{}\".format(i+1) for i in range(num_sub_datasets)]\n",
    "metric = calculate_bagging_f1(model_names, test_dataloader)\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting results to generate dev.txt and test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function get_dataloaders.<locals>.tokenize_function at 0x7f7685575820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1561e643024298a91cf2b3b0f5c48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing dev-generated.txt\n"
     ]
    }
   ],
   "source": [
    "# generate dev.txt from test_data.csv\n",
    "_, test_dataloader = get_dataloaders(args, None, test_data)\n",
    "\n",
    "model_names = [\"bagging_model_{}\".format(i+1) for i in range(num_sub_datasets)]\n",
    "y_pred = predict(model_names, test_dataloader)\n",
    "# write into dev-generated.txt, with 1 label per line\n",
    "print(\"Start writing dev-generated.txt\")\n",
    "with open('dev-generated.txt', 'w') as f:\n",
    "    for label in y_pred:\n",
    "        f.write(str(label))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6bfddd71b04eec81848f7349169789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing test-generated.txt\n"
     ]
    }
   ],
   "source": [
    "# generate test.txt from final_pred.csv\n",
    "_, final_pred_dataloader = get_dataloaders(args, None, final_pred_data)\n",
    "\n",
    "model_names = [\"bagging_model_{}\".format(i+1) for i in range(num_sub_datasets)]\n",
    "y_pred = predict(model_names, final_pred_dataloader)\n",
    "# write into test-generated.txt, with 1 label per line\n",
    "print(\"Start writing test-generated.txt\")\n",
    "with open('test-generated.txt', 'w') as f:\n",
    "    for label in y_pred:\n",
    "        f.write(str(label))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "# checking the class of the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bagging_model_1\", num_labels=2)\n",
    "print(type(model))\n",
    "\n",
    "# # load y_pred from dev.txt\n",
    "# y_pred = np.loadtxt('dev.txt', dtype=int)\n",
    "# print(y_pred)\n",
    "\n",
    "# # Calculate the f1 score again, using the same metric\n",
    "# print(\"Bagging f1 score\")\n",
    "# model_names = [\"bagging_model_{}\".format(i+1) for i in range(num_sub_datasets)]\n",
    "# # Get the labels\n",
    "# y_true = np.array([])\n",
    "# for batch in test_dataloader:\n",
    "#     y_true = np.concatenate((y_true, batch['labels'].numpy()))\n",
    "# # Calculate the f1 score\n",
    "# metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "# result_dict = metric.compute(predictions=y_pred, references=y_true)\n",
    "# print(result_dict[\"f1\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf3454d8134cf31c53930b86a97c38f4c38d2a7205730a8063b2bb452273ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
