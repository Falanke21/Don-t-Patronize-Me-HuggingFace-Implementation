{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from train import get_dataloaders, train_model, calculate_f1\n",
    "\n",
    "import logging\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_data = pd.read_csv(data_dir + 'train_data.csv')\n",
    "#augmented_train_data = pd.read_csv(data_dir + 'augmented_data_label_1_pegasus.csv')\n",
    "augmented_train_data = pd.read_csv(data_dir + 'augmented_data_label_1_parrot.csv')\n",
    "train_data = pd.concat([original_train_data, augmented_train_data], axis=0)\n",
    "\n",
    "test_data = pd.read_csv(data_dir + 'test_data.csv')\n",
    "final_pred_data = pd.read_csv(data_dir + 'final_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 5 balanced sub-datasets for bagging\n",
    "num_sub_datasets = 5\n",
    "train_data_list = []\n",
    "\n",
    "ones = train_data[train_data['label'] == 1]\n",
    "zeros = train_data[train_data['label'] == 0]\n",
    "num = len(ones)\n",
    "\n",
    "for i in range(num_sub_datasets):\n",
    "    train_data_list.append(pd.concat([ones, zeros.sample(n=num, random_state=i)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's any null value in the text column\n",
    "test_data[test_data['text'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO\n",
    "1. train the same model on 5 sub-datasets.\n",
    "2. predict labels using the 5 models (voting).\n",
    "3. calculate the accuracy and f1 score of the voting result.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "USE_LR_SCHEDULER = False\n",
    "PRETRAINED_MODEL_NAME = \"roberta-base\"\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "args = {\n",
    "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "    \"USE_LR_SCHEDULER\": USE_LR_SCHEDULER,\n",
    "    \"PRETRAINED_MODEL_NAME\": PRETRAINED_MODEL_NAME,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels using the 5 models (voting)\n",
    "def predict(model_names, test_dataloader):\n",
    "    y_pred_list = []\n",
    "    for name in model_names:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(name, num_labels=2)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        # The list of predictions for each model\n",
    "        y_pred = []\n",
    "        for batch in test_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            # Extend every batch\n",
    "            y_pred.extend(logits.argmax(-1).cpu().numpy())\n",
    "        # Now we have a list of list of predictions\n",
    "        y_pred_list.append(y_pred)\n",
    "    # Voting\n",
    "    y_pred = np.array(y_pred_list).T\n",
    "    y_pred = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=y_pred)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def calculate_bagging_f1(model_names, val_dataloader):\n",
    "    # Get the predictions\n",
    "    y_pred = predict(model_names, val_dataloader)\n",
    "    # Get the labels\n",
    "    y_true = np.array([])\n",
    "    for batch in val_dataloader:\n",
    "        y_true = np.concatenate((y_true, batch['labels'].numpy()))\n",
    "    # Calculate the f1 score\n",
    "    metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "    result_dict = metric.compute(predictions=y_pred, references=y_true)\n",
    "    return result_dict[\"f1\"]\n",
    "\n",
    "# If you need to load the models from disk here's your helper\n",
    "def load_models(num_models: int, device: torch.device):\n",
    "    model_list = []\n",
    "    for i in range(num_models):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(f\"bagging_model_{i+1}\", num_labels=2)\n",
    "        model.to(device)\n",
    "        model_list.append(model)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our \"bags\"\n",
    "print('Training {} models...'.format(num_sub_datasets))\n",
    "for i in range(num_sub_datasets):\n",
    "    print('Training model {}...'.format(i+1))\n",
    "    train_dataloader, _ = get_dataloaders(args, train_data_list[i], None)\n",
    "    train_model(args, device, train_dataloader, model_name = \"bagging_model_{}\".format(i+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d6d573d9c84db692b05162860f3d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a3dc51cc6a4da494848f4672345de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate f1 score\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897d7f89d5154fe18d77da46ac1ad13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e272e1ec25e401cbf439eec4ffe5589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471156d784924c25b58fac4c4b88c155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bc112059cb4975858bf2fc6543c8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47510668563300146, 0.5707317073170732, 0.532687651331719, 0.4889543446244477, 0.5203619909502262]\n",
      "Bagging f1 score\n",
      "0.5708418891170431\n"
     ]
    }
   ],
   "source": [
    "_, test_dataloader = get_dataloaders(args, None, test_data)\n",
    "\n",
    "print(\"Separate f1 score\")\n",
    "list_of_metrics = []\n",
    "for i in range(num_sub_datasets):\n",
    "    metric = calculate_f1(f\"bagging_model_{i+1}\", device, test_dataloader)\n",
    "    list_of_metrics.append(metric)\n",
    "print(list_of_metrics)\n",
    "print(\"Bagging f1 score\")\n",
    "model_names = [\"bagging_model_{}\".format(i+1) for i in range(num_sub_datasets)]\n",
    "metric = calculate_bagging_f1(model_names, test_dataloader)\n",
    "print(metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting results to generate dev.txt and test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ef0af54fb1454dbb7e222aa28409b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5269e8d29b5e498c922c194ab6397c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing test-generated.txt\n"
     ]
    }
   ],
   "source": [
    "# generate dev.txt from test_data.csv\n",
    "_, test_dataloader = get_dataloaders(args, None, test_data)\n",
    "\n",
    "model_names = [\"bagging_model_{}\".format(i+1) for i in range(num_sub_datasets)]\n",
    "y_pred = predict(model_names, test_dataloader)\n",
    "# write into dev-generated.txt, with 1 label per line\n",
    "print(\"Start writing dev-generated.txt\")\n",
    "with open('dev-generated.txt', 'w') as f:\n",
    "    for label in y_pred:\n",
    "        f.write(str(label))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function get_dataloaders.<locals>.tokenize_function at 0x7f7b1a853f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793ccb424579444cb32e5cdbca43b9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing test-generated.txt\n"
     ]
    }
   ],
   "source": [
    "# generate test.txt from final_pred.csv\n",
    "_, final_pred_dataloader = get_dataloaders(args, None, final_pred_data)\n",
    "\n",
    "model_names = [\"bagging_model_{}\".format(i+1) for i in range(num_sub_datasets)]\n",
    "y_pred = predict(model_names, final_pred_dataloader)\n",
    "# write into test-generated.txt, with 1 label per line\n",
    "print(\"Start writing test-generated.txt\")\n",
    "with open('test-generated.txt', 'w') as f:\n",
    "    for label in y_pred:\n",
    "        f.write(str(label))\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9bf3454d8134cf31c53930b86a97c38f4c38d2a7205730a8063b2bb452273ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
